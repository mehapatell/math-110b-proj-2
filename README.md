# math-110b-proj-2
Testing a 4 layer Feed-forward Neural Network (FNN) using mini-batch stochastic gradient decent. This examples includes a testing, training, and cross-validation set. 

This project is derived from the following starter code https://www.kaggle.com/code/scaomath/simple-mnist-numpy-from-scratch/notebook.

This project highlights the following key concepts:
 - chain rule
 - gradient descent
 - probability distribution
 - intro to neural networks
 - loss function
